llm:
  provider: api-endpoint

api-endpoint:
  api_base: http://localhost:11434/v1
  api_key: ollama
  model: gpt-oss:20b
  max_retries: 3
  retry_delay: 1.0
  sleep_time: 0.5

generation:
  temperature: 0.0  # Crucial: 0.0 minimizes hallucination in 20B models
  top_p: 0.9
  max_tokens: 4096

prompts:
  summary: |
    Briefly summarize the clinical text below.

  qa_generation: |
    You are a clinical educator and SFT dataset architect. Your task is to transform the provided medical text into high-fidelity QA pairs.

    ### ADAPTIVE REQUIREMENTS:
    1. **DYNAMIC QUANTITY & DEPTH**: 
       - Adjust volume based on content: 1-2 pairs for simple summaries; up to {num_pairs} for dense guidelines.
       - Match the depth to the complexity of the topic (e.g., brief for definitions, long-form for management protocols).

    2. **STRICT GROUNDING & EVIDENCE**: 
       - Use ONLY numbers, dosages, and statistics found in the text. 
       - You MUST explicitly include any evidence markers (e.g., Grade C, Level 2+, P) to justify the clinical reasoning in the answer.

    3. **INTEGRATED CLINICAL REASONING**: 
       - Avoid "fragmented" questions. Instead of asking for a single fact, synthesize related points. (e.g., Combine a patient's risk profile with the recommended intervention).
       - Frame scenarios: "A patient presents with X and is concerned about Y. Based on the guideline, how should the clinician balance these risks?"

    4. **NO EXTERNAL KNOWLEDGE**: Do not pull from outside training data. If a dosage or specific drug isn't in the text, do not mention it.

    **FORMATTING**: Output strictly as a JSON list of objects: [{"question": "...", "answer": "..."}].

    ---
    TEXT TO PROCESS:
    {text}
    
    Output strictly in JSON format.
